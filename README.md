# Reinforcement Learning Agent for Atari Games

Ce projet pr√©sente une impl√©mentation d'un agent d'apprentissage par renforcement profond bas√© sur l'algorithme DQN (Deep Q-Network) pour jouer √† des jeux Atari. Le code est d√©velopp√© en utilisant **PyTorch** et **Gymnasium**.

L'impl√©mentation int√®gre plusieurs am√©liorations cl√©s inspir√©es des recherches de DeepMind pour am√©liorer la performance et la stabilit√© de l'entra√Ænement, notamment :

  * **Double DQN** pour r√©duire la sur√©valuation des Q-values.
  * Une architecture de r√©seau **Dueling DQN** pour mieux estimer la valeur des √©tats.
  * Un traitement avanc√© des observations (Frame Skipping et Frame Stacking).
  * Un Replay Buffer pour d√©corr√©ler les exp√©riences pass√©es.

## R√©sultats Obtenus

Voici un aper√ßu des performances de l'agent apr√®s un entra√Ænement complet.

**M√©triques d'entra√Ænement finales :**

![M√©triques d'entra√Ænement finales](doc/metrics_entrainement_final.png)

**Vid√©o de la meilleure ex√©cution :**

![](https://github.com/user-attachments/assets/70c6d93e-b58c-4e1b-bc1c-d466cca17088)

-----

## ‚öôÔ∏è Installation

Suivez ces √©tapes pour configurer votre environnement et installer les d√©pendances n√©cessaires.

### Pr√©requis

  * Git
  * Conda (ou Miniconda)

### √âtapes d'installation

1.  **Cloner le d√©p√¥t**

    ```bash
    git clone git@github.com:ArrnaudMoreau22/reinforcement_learning_atari.git
    cd reinforcement_learning_atari
    ```

2.  **Cr√©er et activer l'environnement Conda**
    Je recommande d'utiliser un environnement virtuel pour isoler les d√©pendances du projet.

    ```bash
    # Cr√©e un environnement nomm√© 'rl_gymnasium' avec Python 3.9
    conda create -n rl_gymnasium python=3.9

    # Active l'environnement
    conda activate rl_gymnasium
    ```

3.  **Installer les d√©pendances**
    Le fichier `requirement.txt` contient toutes les biblioth√®ques Python n√©cessaires.

    ```bash
    pip install -r requirement.txt
    ```

    Cela installera les biblioth√®ques principales telles que `torch`, `gymnasium[atari]`, `ale-py`, et `numpy`.

-----

## üöÄ Lancement de l'entra√Ænement

Le script principal pour lancer l'entra√Ænement est `run.py`. Vous pouvez personnaliser les hyperparam√®tres via les arguments de la ligne de commande.

### Commande de base

Pour lancer un entra√Ænement sur l'environnement Pong avec les param√®tres par d√©faut :

```bash
python run.py --env "ALE/Pong-v5"
```

### Arguments personnalis√©s

Vous pouvez ajuster les param√®tres de l'entra√Ænement. Par exemple, pour un entra√Ænement plus court sur Breakout avec un *learning rate* diff√©rent :

```bash
python run.py \
    --env "ALE/Breakout-v5" \
    --total-steps 1000000 \
    --lr 0.0001 \
    --buffer-cap 100000 \
    --save-path "checkpoints/dqn_breakout_test.pth"
```

Quelques arguments cl√©s :

  * `--env`: L'ID de l'environnement Gymnasium (ex: `"ALE/Pong-v5"`).
  * `--total-steps`: Le nombre total de pas d'environnement pour l'entra√Ænement (d√©faut: 50M).
  * `--lr`: Le *learning rate* de l'optimiseur RMSprop (d√©faut: 0.00025).
  * `--batch-size`: La taille du batch √©chantillonn√© depuis le Replay Buffer (d√©faut: 32).
  * `--resume`: Chemin vers un checkpoint pour reprendre un entra√Ænement (ex: `"checkpoints/dqn_breakout.pth"`).

-----

## üìÇ Structure du Projet

Le projet est organis√© en plusieurs fichiers modulaires :

  * `run.py`: Point d'entr√©e pour lancer l'entra√Ænement. G√®re les arguments de la ligne de commande.
  * `trainer.py`: Contient la boucle principale d'entra√Ænement, la gestion de l'environnement, le monitoring et la sauvegarde.
  * `model.py`: D√©finit l'architecture du r√©seau de neurones (Dueling DQN).
  * `train_step.py`: Impl√©mente une seule √©tape de mise √† jour des poids du r√©seau (calcul de la perte et backpropagation) en utilisant la logique Double DQN.
  * `agent.utils.py`: Fournit des fonctions utilitaires pour l'agent, comme la s√©lection d'action Œµ-greedy.
  * `replay_buffer.py`: Classe pour le buffer de rejouabilit√© qui stocke les transitions `(s, a, r, s')`.
  * `preprocessing.py`: Contient les classes de pr√©traitement, notamment `FrameStack` pour empiler les observations.
  * `requirement.txt`: Liste des d√©pendances Python du projet.
  * `doc/`: Dossier contenant des ressources et illustrations sur les concepts cl√©s.

-----

## üß† Concepts Cl√©s Impl√©ment√©s

Ce projet s'appuie sur plusieurs techniques fondamentales de l'apprentissage par renforcement profond.

### Frame Skipping & Frame Stacking

Pour des raisons d'efficacit√© et pour fournir un contexte temporel √† l'agent, j'utilise deux techniques :

![Sch√©ma Frame Skipping & Frame Stacking](doc/schema_frameskip_framestack.jpg)

1.  **Frame Skipping** : Le wrapper `AtariPreprocessing` est configur√© avec `frame_skip=4`. L'agent ex√©cute la m√™me action pendant 4 images cons√©cutives et ne traite que la derni√®re, ce qui acc√©l√®re l'entra√Ænement.
2.  **Frame Stacking** : Pour que l'agent puisse percevoir le mouvement (ex: la direction de la balle), j'empile les 4 derni√®res images pr√©trait√©es en un seul tenseur d'√©tat `(4, 84, 84)`. Ce tenseur est ensuite donn√© en entr√©e du r√©seau de neurones.

### Architecture DQN Vanilla

Avant d'aborder les am√©liorations apport√©es par le Double DQN et le Dueling DQN, il est important de comprendre l'architecture de base du DQN (Deep Q-Network) classique, telle que pr√©sent√©e dans l'√©tude fondatrice de DeepMind *["Human-level control through deep reinforcement learning"](https://arxiv.org/abs/1312.5602)* (Mnih et al., 2015) :

![Architecture DQN Vanilla](doc/dqn_vanilla.png)

L'architecture Vanilla DQN utilise un r√©seau de neurones convolutionnel qui :

1.  **Traite les images** : Les couches convolutionnelles extraient les caract√©ristiques spatiales des frames empil√©es (4 x 84 x 84).
2.  **Apprend les repr√©sentations** : Les couches denses transforment ces caract√©ristiques en repr√©sentations de haut niveau.
3.  **Estime les Q-values** : La couche de sortie produit directement une Q-value pour chaque action possible.

Cette architecture simple mais r√©volutionnaire a permis pour la premi√®re fois d'atteindre des performances humaines sur plusieurs jeux Atari et constitue la base sur laquelle toutes les am√©liorations ult√©rieures sont construites.

### Double DQN

L'algorithme DQN classique souffre d'une sur√©valuation des Q-values car il utilise le m√™me r√©seau (le *target network*) pour **s√©lectionner** la meilleure action future et pour **√©valuer** sa valeur.

**Double DQN** corrige ce biais. Comme impl√©ment√© dans `train_step.py`, j'utilise :

![Sch√©ma Train Step - Double DQN vs Vanilla DQN](doc/TrainStep_DoubleDQN_VS_VanillaDQN.png)

1.  Le **r√©seau principal (policy\_net)** pour choisir la meilleure action pour l'√©tat suivant (`argmax a' Q_policy(s', a')`).
2.  Le **r√©seau cible (target\_net)** pour √©valuer la Q-value de cette action choisie (`Q_target(s', argmax a' Q_policy(s', a'))`).

Cette d√©corr√©lation entre la s√©lection et l'√©valuation m√®ne √† des estimations de valeur plus pr√©cises et √† un entra√Ænement plus stable.

### Dueling DQN

L'architecture Dueling DQN, d√©finie dans `model.py`, modifie la structure du r√©seau pour estimer deux quantit√©s s√©par√©es :

![Architecture Dueling DQN](doc/model_dueling_dqn.png)

1.  **La Valeur de l'√âtat (V(s))** : Une sortie unique qui estime la qualit√© de l'√©tat `s`, ind√©pendamment de l'action choisie.
2.  **L'Avantage par Action (A(s, a))** : Une sortie pour chaque action qui repr√©sente l'avantage de choisir une action `a` par rapport aux autres dans cet √©tat `s`.

Ces deux flux sont ensuite combin√©s pour produire les Q-values finales. Cette s√©paration permet au mod√®le d'apprendre plus efficacement quels √©tats sont pr√©cieux sans avoir √† apprendre l'effet de chaque action pour chaque √©tat.
H
### Replay Buffer

Le Replay Buffer est un composant essentiel qui stocke les exp√©riences pass√©es de l'agent sous forme de transitions `(√©tat, action, r√©compense, √©tat_suivant)`. Cette technique, introduite avec le DQN original, permet de d√©corr√©ler les √©chantillons d'entra√Ænement et d'am√©liorer la stabilit√© de l'apprentissage.

**Configuration par d√©faut :**
- **Taille du buffer** : 500 000 transitions (param√®tre `--buffer-cap`)
- **Taille des √©chantillons** : 32 transitions par batch (param√®tre `--batch-size`)

**Consid√©rations importantes pour la m√©moire :**

Chaque transition stocke 4 frames de 84x84 pixels pour l'√©tat actuel et l'√©tat suivant, ce qui repr√©sente environ 56 KB par transition. Avec 500 000 transitions, le buffer peut consommer jusqu'√† **28 GB de RAM**.

**Recommandations selon votre configuration :**
- **16 GB de RAM** : R√©duisez √† `--buffer-cap 200000` (~11 GB)
- **32 GB de RAM ou plus** : Vous pouvez conserver la taille par d√©faut (~28 GB)
- **64 GB de RAM ou plus** : Vous pouvez augmenter √† `--buffer-cap 1_000_000` (~56 GB)

Pour ajuster la taille du buffer lors de l'entra√Ænement :
```bash
python run.py --env "ALE/Pong-v5" --buffer-cap 200000
```

**Note :** Un buffer plus petit peut l√©g√®rement r√©duire les performances, mais reste pr√©f√©rable √† un crash par manque de m√©moire (OOM - Out Of Memory).

### Wrappers d'Environnement

Pour g√©rer les sp√©cificit√©s des jeux Atari, plusieurs wrappers `Gymnasium` sont utilis√©s dans `trainer.py`:

  * `AtariPreprocessing`: Effectue le pr√©traitement standard : redimensionne l'image en 84x84, la convertit en niveaux de gris, et g√®re le *frame-skipping*. L'option `terminal_on_life_loss=False` est activ√©e pour que l'agent apprenne que perdre une vie n'est pas la fin de l'√©pisode.
  * `NoopResetEnv`: Commence chaque √©pisode par un nombre al√©atoire d'actions "NO-OP" (ne rien faire) pour introduire de la stochasticit√© dans les conditions de d√©part.
  * `FireResetEnv` / `FireOnLifeLossEnv`: Pour les jeux comme Breakout, ces wrappers (actuellement comment√©s mais disponibles) envoient automatiquement l'action "FIRE" pour d√©marrer une partie ou apr√®s avoir perdu une vie.

-----

## üõ†Ô∏è Troubleshooting

### Probl√®mes courants et solutions

**üî• Erreur "Out of Memory" (OOM)**
```
RuntimeError: CUDA out of memory
```
**Solutions :**
- R√©duisez la taille du replay buffer : `--buffer-cap 200000`
- Diminuez la taille du batch : `--batch-size 16`
- Utilisez un GPU avec plus de VRAM ou entra√Ænez sur CPU

**üêå Entra√Ænement tr√®s lent**
**Solutions :**
- V√©rifiez que PyTorch utilise bien le GPU : `torch.cuda.is_available()`
- R√©duisez le nombre total de steps pour un test : `--total-steps 1000000`
- Augmentez la fr√©quence de sauvegarde : surveillez le monitoring

**‚ùå Erreur d'environnement Atari**
```
gym.error.NamespaceError: Namespace ALE not found
```
**Solutions :**
- R√©installez les d√©pendances Atari : `pip install gymnasium[atari]`
- Installez ALE : `pip install ale-py`

**üìâ L'agent n'apprend pas / performances stagnantes**
**Solutions :**
- V√©rifiez que epsilon d√©cro√Æt correctement dans les logs
- Augmentez le learning rate : `--lr 0.0005`
- Laissez plus de temps d'entra√Ænement (50M steps minimum)
- V√©rifiez que le replay buffer se remplit avant l'entra√Ænement

## üìö R√©f√©rences et Lectures Compl√©mentaires

### Papers Fondamentaux

- **DQN Original** : [Human-level control through deep reinforcement learning](https://arxiv.org/abs/1312.5602) (Mnih et al., 2015)
- **Double DQN** : [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461) (van Hasselt et al., 2016)
- **Dueling DQN** : [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581) (Wang et al., 2016)

### Ressources Suppl√©mentaires

- **Cours OpenAI Spinning Up** : [spinningup.openai.com](https://spinningup.openai.com/en/latest/) - Guide complet sur l'apprentissage par renforcement
- **Documentation Gymnasium** : [gymnasium.farama.org](https://gymnasium.farama.org/) - Documentation officielle des environnements
- **PyTorch RL Tutorials** : [pytorch.org/tutorials](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) - Tutoriels officiels PyTorch sur le RL

### Am√©liorations Possibles

Ce projet peut √™tre √©tendu avec d'autres techniques avanc√©es :
- **Prioritized Experience Replay** : √âchantillonnage intelligent du replay buffer
- **Rainbow DQN** : Combinaison de plusieurs am√©liorations DQN
- **Distributional RL** : Apprentissage de la distribution des r√©compenses
- **Noisy Networks** : Exploration par bruit param√©trique

## üôè Remerciements

Ce projet s'inspire directement des recherches r√©volutionnaires de **DeepMind** en apprentissage par renforcement profond. Je remercie particuli√®rement :

- **Volodymyr Mnih** et l'√©quipe DeepMind pour l'invention du DQN
- **Hado van Hasselt** pour l'algorithme Double DQN
- **Ziyu Wang** pour l'architecture Dueling DQN
- La communaut√© **OpenAI Gymnasium** pour les environnements Atari
- L'√©quipe **PyTorch** pour le framework de deep learning
- **Epitech** pour ce sujet passionnant et cette opportunit√© d'explorer l'apprentissage par renforcement profond

Ce travail d√©montre la puissance des techniques d'apprentissage par renforcement profond et leur capacit√© √† r√©soudre des t√¢ches complexes de contr√¥le s√©quentiel.

## ‚≠ê Vous avez trouv√© ce projet utile ?

Si ce repository vous a √©t√© utile ou informatif, n'h√©sitez pas √† lui donner une √©toile ! ‚≠ê

Cela m'aide √©norm√©ment et encourage √† continuer le d√©veloppement de projets √©ducatifs en IA et apprentissage par renforcement.

[‚≠ê Donner une √©toile au repository](https://github.com/ArrnaudMoreau22/reinforcement_learning_atari)

---

*D√©velopp√© avec ‚ù§Ô∏è pour l'apprentissage et la recherche en IA*
